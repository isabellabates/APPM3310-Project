{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e3bfe3f7-dd3e-447b-8200-bd6b15ed85a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analysis of Taylor Swift lyrics using SVD\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn import decomposition\n",
    "from scipy import linalg\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, NMF, PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c25d318a-a35e-4479-a3ab-36a68576a202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from file\n",
    "tay = pd.read_csv('TaylorSwiftLyricsFeatureSet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fc630e66-0499-4a87-a1bc-1fb52d003443",
   "metadata": {},
   "outputs": [],
   "source": [
    "tay['year_released'] = tay['track_album']\n",
    "tay['year_released'] = tay['year_released'].replace(['Taylor Swift', 'Fearless', 'Speak Now', 'Red', '1989',\n",
    "       'reputation', 'Lover'],[2006,2008,2010,2012,2014,2017,2019])\n",
    "tay['world_sales_USD'] = tay['track_album'].replace(['Taylor Swift', 'Fearless', 'Speak Now', 'Red', '1989',\n",
    "       'reputation', 'Lover'],[7000000,12000000,5500000,6000000,10500000,5000000,4000000])\n",
    "song_titles = tay['track_title'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ec26c612-bc3b-438b-8707-59e09ded916c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'he said the way my blue eyes shined put those georgia stars to shame that night i said   that s a lie  just a boy in a chevy truck that had a tendency of gettin  stuck on backroads at night and i was right there beside him all summer long and then the time we woke up to find that summer gone but when you think tim mcgraw i hope you think my favorite song the one we danced to all night long the moon like a spotlight on the lake when you think happiness i hope you think that little black dress think of my head on your chest and my old faded blue jeans when you think tim mcgraw i hope you think of me september saw a month of tears and thankin  god that you weren t here to see me like that but in a box beneath my bed is a letter that you never read from three summers back it s hard not to find it all a little bittersweet and lookin  back on all of that  it s nice to believe when you think tim mcgraw i hope you think my favorite song the one we danced to all night long the moon like a spotlight on the lake when you think happiness i hope you think that little black dress think of my head on your chest and my old faded blue jeans when you think tim mcgraw i hope you think of me and i m back for the first time since then i m standin  on your street and there s a letter left on your doorstep and the first thing that you ll read is   when you think tim mcgraw i hope you think my favorite song someday you ll turn your radio on i hope it takes you back to that place  when you think happiness i hope you think that little black dress think of my head on your chest and my old faded blue jeans when you think tim mcgraw i hope you think of me oh  think of me mmmm he said the way my blue eyes shine put those georgia stars to shame that night i said   that s a lie '"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean data before applying NMF\n",
    "# remove \\n\n",
    "n = lambda x: re.sub('\\n',' ',x)\n",
    "alpha = lambda x: re.sub('\\w*\\d\\w*', ' ', x)\n",
    "# remove punctuation and make all lyrics lowercase\n",
    "lowercase = lambda x: re.sub('[%s]' % re.escape(string.punctuation), ' ', x.lower())\n",
    "\n",
    "tay['track_lyric'] = tay.track_lyric.map(n).map(lowercase).map(alpha)\n",
    "\n",
    "# data after cleaning\n",
    "tay.track_lyric[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0581e0b6-664c-4b5f-80b5-abb86cea8ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove words that we don't want to include in analysis \n",
    "stop_words = ['just','don','gonna','cause','ll','ve','got','oh','eh','aah','want','way','away','ooh','wanna','ain','yeah','hey', 'did']\n",
    "remove = text.ENGLISH_STOP_WORDS.union(stop_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8971d30b-767b-48ec-9ac8-be2fd83895b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'stop_words' parameter of CountVectorizer must be a str among {'english'}, an instance of 'list' or None. Got frozenset({'bottom', 'many', 'oh', 'their', 'before', 'via', 'nothing', 'eg', 'cause', 'thereupon', 'an', 'describe', 'found', 'way', 'sixty', 'sincere', 'and', 'fifty', 'two', 'fifteen', 'they', 'from', 'at', 'a', 'others', 'beforehand', 'eight', 'per', 'we', 'one', 'become', 'de', 'someone', 'hereafter', 'there', 'twenty', 'only', 'forty', 'below', 'when', 'same', 'throughout', 'both', 'name', 'this', 'thru', 'former', 'through', 'upon', 'call', 'up', 'detail', 'ourselves', 'empty', 'anyway', 'still', 'which', 'more', 'mostly', 'co', 'again', 'beside', 'whence', 'who', 'not', 'them', 'whether', 'ooh', 'because', 'being', 'none', 'these', 'ltd', 'six', 'hence', 'latter', 'although', 're', 'within', 'also', 'either', 'neither', 'but', 'do', 'off', 'see', 'should', 'were', 'my', 'own', 'therein', 'seem', 'whom', 'got', 'much', 'yourself', 'itself', 'serious', 'was', 'everything', 'therefore', 'last', 'him', 'seemed', 'between', 'hereupon', 'often', 'however', 'myself', 'out', 'for', 'latterly', 'hundred', 'except', 'hereby', 'elsewhere', 'go', 'could', 'together', 'twelve', 'front', 'than', 'am', 'next', 'of', 'top', 'want', 'nine', 've', 'mill', 'those', 'otherwise', 'been', 'into', 'moreover', 'to', 'sometime', 'every', 'nobody', 'nevertheless', 'full', 'formerly', 'may', 'thin', 'be', 'show', 'himself', 'fire', 'since', 'back', 'where', 'less', 'whose', 'whenever', 'please', 'five', 'just', 'keep', 'too', 'might', 'while', 'give', 'amoungst', 'interest', 'gonna', 'whereby', 'your', 'further', 'amongst', 'had', 'now', 'along', 'ever', 'became', 'whereupon', 'other', 'down', 'never', 'part', 'perhaps', 'its', 'side', 'onto', 'even', 'noone', 'namely', 'inc', 'whereafter', 'everywhere', 'is', 'indeed', 'anything', 'another', 'each', 'don', 'nowhere', 'anywhere', 'll', 'cannot', 'have', 'seems', 'after', 'under', 'us', 'yours', 'will', 'here', 'take', 'system', 'though', 'thick', 'about', 'so', 'couldnt', 'move', 'already', 'can', 'whole', 'our', 'somehow', 'toward', 'towards', 'beyond', 'behind', 'few', 'else', 'everyone', 'without', 'among', 'bill', 'with', 'herself', 'thereafter', 'over', 'aah', 'across', 'etc', 'herein', 'until', 'ours', 'made', 'thus', 'why', 'on', 'i', 'fill', 'his', 'several', 'alone', 'me', 'almost', 'ie', 'are', 'you', 'three', 'eleven', 'the', 'whatever', 'cry', 'must', 'well', 'whither', 'very', 'nor', 'it', 'such', 'yourselves', 'get', 'themselves', 'any', 'becomes', 'somewhere', 'ain', 'third', 'enough', 'something', 'due', 'he', 'amount', 'or', 'against', 'anyhow', 'what', 'hey', 'hers', 'by', 'anyone', 'in', 'least', 'her', 'how', 'becoming', 'find', 'during', 'as', 'eh', 'done', 'four', 'meanwhile', 'away', 'whereas', 'she', 'once', 'un', 'first', 'would', 'thence', 'wherever', 'all', 'most', 'put', 'above', 'that', 'con', 'besides', 'thereby', 'cant', 'has', 'wherein', 'rather', 'did', 'ten', 'seeming', 'mine', 'wanna', 'whoever', 'hasnt', 'no', 'yet', 'around', 'always', 'sometimes', 'then', 'some', 'if', 'afterwards', 'yeah'}) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# TF-IDF vectorizer\u001b[39;00m\n\u001b[1;32m      2\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer(stop_words\u001b[38;5;241m=\u001b[39mremove,min_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,max_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m V \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtay\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_lyric\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m V \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(V\u001b[38;5;241m.\u001b[39mtoarray(), columns\u001b[38;5;241m=\u001b[39mvectorizer\u001b[38;5;241m.\u001b[39mget_feature_names())\n\u001b[1;32m      5\u001b[0m vocab \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(vectorizer\u001b[38;5;241m.\u001b[39mget_feature_names())\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1358\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(raw_documents, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1354\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1355\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIterable over raw text documents expected, string object received.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1356\u001b[0m     )\n\u001b[0;32m-> 1358\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1359\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_ngram_range()\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_for_unused_params()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/base.py:570\u001b[0m, in \u001b[0;36mBaseEstimator._validate_params\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_params\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;124;03m\"\"\"Validate types and values of constructor parameters\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \n\u001b[1;32m    565\u001b[0m \u001b[38;5;124;03m    The expected type and values must be defined in the `_parameter_constraints`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;124;03m    accepted constraints.\u001b[39;00m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 570\u001b[0m     \u001b[43mvalidate_parameter_constraints\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parameter_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcaller_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:97\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[0;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     92\u001b[0m     constraints_str \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     93\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m constraints[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     95\u001b[0m     )\n\u001b[0;32m---> 97\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_val\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    100\u001b[0m )\n",
      "\u001b[0;31mInvalidParameterError\u001b[0m: The 'stop_words' parameter of CountVectorizer must be a str among {'english'}, an instance of 'list' or None. Got frozenset({'bottom', 'many', 'oh', 'their', 'before', 'via', 'nothing', 'eg', 'cause', 'thereupon', 'an', 'describe', 'found', 'way', 'sixty', 'sincere', 'and', 'fifty', 'two', 'fifteen', 'they', 'from', 'at', 'a', 'others', 'beforehand', 'eight', 'per', 'we', 'one', 'become', 'de', 'someone', 'hereafter', 'there', 'twenty', 'only', 'forty', 'below', 'when', 'same', 'throughout', 'both', 'name', 'this', 'thru', 'former', 'through', 'upon', 'call', 'up', 'detail', 'ourselves', 'empty', 'anyway', 'still', 'which', 'more', 'mostly', 'co', 'again', 'beside', 'whence', 'who', 'not', 'them', 'whether', 'ooh', 'because', 'being', 'none', 'these', 'ltd', 'six', 'hence', 'latter', 'although', 're', 'within', 'also', 'either', 'neither', 'but', 'do', 'off', 'see', 'should', 'were', 'my', 'own', 'therein', 'seem', 'whom', 'got', 'much', 'yourself', 'itself', 'serious', 'was', 'everything', 'therefore', 'last', 'him', 'seemed', 'between', 'hereupon', 'often', 'however', 'myself', 'out', 'for', 'latterly', 'hundred', 'except', 'hereby', 'elsewhere', 'go', 'could', 'together', 'twelve', 'front', 'than', 'am', 'next', 'of', 'top', 'want', 'nine', 've', 'mill', 'those', 'otherwise', 'been', 'into', 'moreover', 'to', 'sometime', 'every', 'nobody', 'nevertheless', 'full', 'formerly', 'may', 'thin', 'be', 'show', 'himself', 'fire', 'since', 'back', 'where', 'less', 'whose', 'whenever', 'please', 'five', 'just', 'keep', 'too', 'might', 'while', 'give', 'amoungst', 'interest', 'gonna', 'whereby', 'your', 'further', 'amongst', 'had', 'now', 'along', 'ever', 'became', 'whereupon', 'other', 'down', 'never', 'part', 'perhaps', 'its', 'side', 'onto', 'even', 'noone', 'namely', 'inc', 'whereafter', 'everywhere', 'is', 'indeed', 'anything', 'another', 'each', 'don', 'nowhere', 'anywhere', 'll', 'cannot', 'have', 'seems', 'after', 'under', 'us', 'yours', 'will', 'here', 'take', 'system', 'though', 'thick', 'about', 'so', 'couldnt', 'move', 'already', 'can', 'whole', 'our', 'somehow', 'toward', 'towards', 'beyond', 'behind', 'few', 'else', 'everyone', 'without', 'among', 'bill', 'with', 'herself', 'thereafter', 'over', 'aah', 'across', 'etc', 'herein', 'until', 'ours', 'made', 'thus', 'why', 'on', 'i', 'fill', 'his', 'several', 'alone', 'me', 'almost', 'ie', 'are', 'you', 'three', 'eleven', 'the', 'whatever', 'cry', 'must', 'well', 'whither', 'very', 'nor', 'it', 'such', 'yourselves', 'get', 'themselves', 'any', 'becomes', 'somewhere', 'ain', 'third', 'enough', 'something', 'due', 'he', 'amount', 'or', 'against', 'anyhow', 'what', 'hey', 'hers', 'by', 'anyone', 'in', 'least', 'her', 'how', 'becoming', 'find', 'during', 'as', 'eh', 'done', 'four', 'meanwhile', 'away', 'whereas', 'she', 'once', 'un', 'first', 'would', 'thence', 'wherever', 'all', 'most', 'put', 'above', 'that', 'con', 'besides', 'thereby', 'cant', 'has', 'wherein', 'rather', 'did', 'ten', 'seeming', 'mine', 'wanna', 'whoever', 'hasnt', 'no', 'yet', 'around', 'always', 'sometimes', 'then', 'some', 'if', 'afterwards', 'yeah'}) instead."
     ]
    }
   ],
   "source": [
    "# TF-IDF vectorizer\n",
    "vectorizer = CountVectorizer(stop_words=remove,min_df=0.1,max_df=0.7)\n",
    "V = vectorizer.fit_transform(tay.track_lyric)\n",
    "V = pd.DataFrame(V.toarray(), columns=vectorizer.get_feature_names())\n",
    "vocab = np.array(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b319ad-50d5-4a83-8874-c55aa3db23c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time U, S, Vt = linalg.svd(V, full_matrices=False)\n",
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bf69cb-a5fa-4077-9ec5-ee1d7b5beeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# singular values of s\n",
    "plt.plot(S);\n",
    "xvals = np.linspace(0, 99)\n",
    "yvals = [25 for x in xvals]\n",
    "plt.plot(xvals,yvals)\n",
    "plt.xlabel('Order')\n",
    "plt.ylabel('Singular Values')\n",
    "plt.title('Scree Plot SVD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847a0696-71b8-40e1-884d-582d37d0adad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(S[:16]);\n",
    "plt.xlabel('Order')\n",
    "plt.ylabel('Singular Values')\n",
    "plt.title('Scree Plot SVD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab58e594-68a4-46ed-99ad-88dbbd2ac9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topics found using SVD\n",
    "num_top_words=15;\n",
    "\n",
    "def print_topics(v):\n",
    "    top_words = lambda t: [vocab[i] for i in np.argsort(t)[:-num_top_words-1:-1]]\n",
    "    topic_words = ([top_words(t) for t in v])\n",
    "    return [' '.join(t) for t in topic_words]\n",
    "\n",
    "print_topics(Vt[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f7f35b-5af1-473a-b7d5-9ee401bd0c5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7eb6278-bf38-4c17-b915-98e7681aa1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Resolving problems\n",
    "Connection\n",
    "Short love\n",
    "\n",
    "\n",
    "Passionate, ruthless love\n",
    "Remorse, regret\n",
    "Night out\n",
    "Nostalgia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b4a002-4058-47d9-ba86-965755e3e5fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dab7ea3-55ff-4320-a2c5-353e387d1e30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73518b1-44d4-4e81-a973-0b5a3d6db806",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b05aba2-74c3-49cc-9c76-62352d0e1060",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = \n",
    "['Fights, hard feelings',\n",
    " 'Growing Up, Home',\n",
    " 'Dreams',\n",
    " 'Remembering',\n",
    " 'New good love',\n",
    " 'Twisted love',\n",
    " '7',\n",
    " '8',\n",
    " 'Change']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
